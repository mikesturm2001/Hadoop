{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create Spark session as we did in the class. Make sure you enable Hive support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Lecture_9\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Load all files from \"data\\retail-data\\by-day\" into a single Spark DataFrame. There should be 300+ files in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I moved the the by-day folder to the relative path of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"by-day/*.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Print Schema of the DF created in 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Create SQL temp table to be used in Spark SQL for the current session. Name table \"RetailData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"RetailData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Display list of Hive Tables, permanent and temporary in the \"defualt\" schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "| default| people_pq|      false|\n",
      "| default| sample_07|      false|\n",
      "| default| sample_08|      false|\n",
      "|        |   dftable|       true|\n",
      "|        |retaildata|       true|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Cache the DF so that it wuill reside in memory after first \"action\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Perform count() funnction on the DataFrame to cache the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Use limit function to display first five records using both SQL and DF API. \n",
    "# Full content of each column should be displayed use proper parameter to show() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from RetailData limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Spark API and Spar SQL (two different commands) to find how many records are present in the DF loaded in \"1\"\n",
    "#for every year-month combination. Sort by year-month in default sort order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|count(1)|   Date|\n",
      "+--------+-------+\n",
      "|   84711|2011 11|\n",
      "|   50226|2011 09|\n",
      "|   39518|2011 07|\n",
      "|   25525|2011 12|\n",
      "|   35147|2011 01|\n",
      "|   60742|2011 10|\n",
      "|   35284|2011 08|\n",
      "|   37030|2011 05|\n",
      "|   42481|2010 12|\n",
      "|   27707|2011 02|\n",
      "|   36748|2011 03|\n",
      "|   29916|2011 04|\n",
      "|   36874|2011 06|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*), DATE_FORMAT(InvoiceDate, 'Y MM') as Date from RetailData group by Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  Date|count|\n",
      "+------+-----+\n",
      "|201111|84711|\n",
      "|201110|60742|\n",
      "| 20119|50226|\n",
      "|201012|42481|\n",
      "| 20117|39518|\n",
      "| 20115|37030|\n",
      "| 20116|36874|\n",
      "| 20113|36748|\n",
      "| 20118|35284|\n",
      "| 20111|35147|\n",
      "| 20114|29916|\n",
      "| 20112|27707|\n",
      "|201112|25525|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, year, month\n",
    "df\\\n",
    ".withColumn(\"Year\", year('InvoiceDate'))\\\n",
    ".withColumn(\"Month\", month('InvoiceDate'))\\\n",
    ".withColumn(\"Date\", concat(\"Year\", \"Month\"))\\\n",
    ".groupBy(\"Date\")\\\n",
    ".count()\\\n",
    ".sort('count', ascending=False)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. How many records have Null in field \"Description\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1454"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr,col\n",
    "df.where(df[\"Description\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double check with SQL\n",
    "#spark.sql(\"select count(*) from RetailData where Description is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Create new DF and populate field \"Description\" with 'NULL-STR' string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_col_vals = {\"Description\" : \"NULL-STR\"}\n",
    "df2 = df.na.fill(fill_col_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1454\n"
     ]
    }
   ],
   "source": [
    "print(df2.where(df2[\"Description\"].isNull()).count())\n",
    "print(df2.where(df2[\"Description\"] == \"NULL-STR\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Create new Boolean column for each item in the following list: ['lunch','dinner','supper','breakfast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lower\n",
    "df3 = df2.withColumn(\"is_lunch\", when(instr(lower(df2.Description), \"lunch\") > 0, True).otherwise(False))\\\n",
    ".withColumn(\"is_dinner\", when(instr(lower(df2.Description), \"dinner\") > 0, True).otherwise(False))\\\n",
    ".withColumn(\"is_supper\", when(instr(lower(df2.Description), \"supper\") > 0, True).otherwise(False))\\\n",
    ".withColumn(\"is_breakfast\", when(instr(lower(df2.Description), \"breakfast\") > 0, True).otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Populate the column as in the example below, and output first 50 records that had any of the items \n",
    "#in the item list matched to the content of the column \"Description\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------------+--------------------+\n",
      "|is_lunch|is_dinner|is_supper|is_breakfast|         Description|\n",
      "+--------+---------+---------+------------+--------------------+\n",
      "|   false|    false|    false|        true|FULL ENGLISH BREA...|\n",
      "|    true|    false|    false|       false|LUNCH BAG VINTAGE...|\n",
      "|    true|    false|    false|       false|LUNCH BAG PAISLEY...|\n",
      "|    true|    false|    false|       false|LUNCH BAG SUKI DE...|\n",
      "|    true|    false|    false|       false|CIRCUS PARADE LUN...|\n",
      "|    true|    false|    false|       false|  LUNCH BAG WOODLAND|\n",
      "|    true|    false|    false|       false| SPACEBOY LUNCH BOX |\n",
      "|    true|    false|    false|       false|DOLLY GIRL LUNCH BOX|\n",
      "|    true|    false|    false|       false|  LUNCH BAG WOODLAND|\n",
      "|    true|    false|    false|       false| LUNCH BAG CARS BLUE|\n",
      "|    true|    false|    false|       false|SKULL LUNCH BOX W...|\n",
      "|    true|    false|    false|       false|STRAWBERRY LUNCH ...|\n",
      "|    true|    false|    false|       false|DINOSAUR LUNCH BO...|\n",
      "|    true|    false|    false|       false|LUNCH BOX WITH CU...|\n",
      "|    true|    false|    false|       false|LUNCH BAG PINK PO...|\n",
      "|   false|     true|    false|       false|TV DINNER TRAY DO...|\n",
      "|   false|     true|    false|       false|SPACEBOY TV DINNE...|\n",
      "|    true|    false|    false|       false|DOLLY GIRL LUNCH BOX|\n",
      "|   false|     true|    false|       false|4 SKY BLUE DINNER...|\n",
      "|    true|    false|    false|       false|LUNCH BAG  BLACK ...|\n",
      "|    true|    false|    false|       false|LUNCH BAG  BLACK ...|\n",
      "|    true|    false|    false|       false|LUNCH BAG APPLE D...|\n",
      "|   false|     true|    false|       false|TV DINNER TRAY AI...|\n",
      "|   false|     true|    false|       false|SPACEBOY TV DINNE...|\n",
      "|   false|     true|    false|       false|SKULL DESIGN TV D...|\n",
      "|    true|    false|    false|       false| SPACEBOY LUNCH BOX |\n",
      "|    true|    false|    false|       false|DOLLY GIRL LUNCH BOX|\n",
      "|   false|     true|    false|       false|4 SKY BLUE DINNER...|\n",
      "|    true|    false|    false|       false|  LUNCH BAG WOODLAND|\n",
      "|    true|    false|    false|       false| LUNCH BAG CARS BLUE|\n",
      "|    true|    false|    false|       false|SKULL LUNCH BOX W...|\n",
      "|    true|    false|    false|       false|STRAWBERRY LUNCH ...|\n",
      "|    true|    false|    false|       false|LUNCH BOX WITH CU...|\n",
      "|    true|    false|    false|       false|LUNCH BAG PINK PO...|\n",
      "|    true|    false|    false|       false|LUNCH BAG RED VIN...|\n",
      "|   false|    false|    false|        true|FULL ENGLISH BREA...|\n",
      "|   false|     true|    false|       false|TV DINNER TRAY AI...|\n",
      "|   false|    false|    false|        true|CHILDS BREAKFAST ...|\n",
      "|    true|    false|    false|       false|LUNCH BAG RED RET...|\n",
      "|    true|    false|    false|       false|LUNCH BAG VINTAGE...|\n",
      "|   false|     true|    false|       false|TV DINNER TRAY DO...|\n",
      "|   false|     true|    false|       false|SPACEBOY TV DINNE...|\n",
      "|    true|    false|    false|       false|  LUNCH BAG WOODLAND|\n",
      "|    true|    false|    false|       false|LUNCH BAG VINTAGE...|\n",
      "|    true|    false|    false|       false|LUNCH BAG SPACEBO...|\n",
      "|    true|    false|    false|       false|LUNCH BAG  BLACK ...|\n",
      "|    true|    false|    false|       false|SKULL LUNCH BOX W...|\n",
      "|   false|     true|    false|       false|SPACEBOY TV DINNE...|\n",
      "|   false|     true|    false|       false|4 IVORY DINNER CA...|\n",
      "|   false|     true|    false|       false|4 PINK DINNER CAN...|\n",
      "+--------+---------+---------+------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_of_columns = ['is_lunch','is_dinner','is_supper','is_breakfast','Description']\n",
    "df3.select(list_of_columns)\\\n",
    ".where((df3.is_lunch == True) | (df3.is_dinner == True) | (df3.is_supper == True) | (df3.is_breakfast == True)).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Output frequencies for each Boolean column you have added in 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|is_lunch| count|\n",
      "+--------+------+\n",
      "|    true| 18525|\n",
      "|   false|523384|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3\\\n",
    ".groupBy(\"is_lunch\")\\\n",
    ".count()\\\n",
    ".sort('count')\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|is_dinner| count|\n",
      "+---------+------+\n",
      "|     true|  1746|\n",
      "|    false|540163|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3\\\n",
    ".groupBy(\"is_dinner\")\\\n",
    ".count()\\\n",
    ".sort('count')\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|is_supper| count|\n",
      "+---------+------+\n",
      "|    false|541909|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3\\\n",
    ".groupBy(\"is_supper\")\\\n",
    ".count()\\\n",
    ".sort('count')\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|is_breakfast| count|\n",
      "+------------+------+\n",
      "|        true|  1171|\n",
      "|       false|540738|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3\\\n",
    ".groupBy(\"is_breakfast\")\\\n",
    ".count()\\\n",
    ".sort('count')\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
